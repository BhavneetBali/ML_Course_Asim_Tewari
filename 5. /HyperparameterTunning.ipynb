{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Machine Learning model is defined as a mathematical model with several parameters that need to be learned from the data.\n",
    "\n",
    "- By training a model with existing data, we can fit the model parameters.\n",
    "\n",
    "-  However, there is another kind of parameter, known as `Hyperparameters`, that cannot be directly learned from the regular training process.\n",
    "  \n",
    "-  They are usually fixed before the actual training process begins.\n",
    "  \n",
    "-  These parameters express important properties of the model such as its *complexity* or *how fast* it should learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameter tuning is the <u>process of selecting the optimal values for a machine learning modelâ€™s hyperparameters.</u>\n",
    "\n",
    "- Hyperparameters are settings that control the learning process of the model, such as the **learning rate**, the **number of neurons** in a neural network, or the **kernel size** in a support vector machine.\n",
    "\n",
    "- The goal of hyperparameter tuning is to find the values that lead to the best performance on a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameters in machine learning are configuration variables set before model training.\n",
    "  \n",
    "- They control the learning process, unlike model parameters learned from the data.\n",
    "  \n",
    "- Hyperparameters are crucial for tuning a model's performance and can impact accuracy, generalization, and other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Ways of Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameters differ from model parameters (weights and biases) learned from the data.\n",
    "  \n",
    "- Various types of hyperparameters exist, each with specific roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters in Neural Networks\n",
    "| Hyperparameter           | Description                                                                                                                |\n",
    "|--------------------------|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Learning rate            | Controls the step size taken by the optimizer during each training iteration. Too small or large rates can lead to convergence issues.           |\n",
    "| Epochs                   | Represents the number of times the entire training dataset passes through the model during training. Increased epochs may enhance performance but could lead to overfitting.    |\n",
    "| Number of layers         | Determines the depth of the model, impacting complexity and learning ability.                                              |\n",
    "| Number of nodes per layer| Influences the width of the model, affecting its capacity to represent complex relationships in the data.                   |\n",
    "| Architecture             | Dictates the overall structure of the neural network, including the number of layers, neurons per layer, and connections. Optimal architecture depends on task complexity and dataset size. |\n",
    "| Activation function      | Introduces non-linearity, enabling the model to learn complex decision boundaries. Common functions include sigmoid, tanh, and Rectified Linear Unit (ReLU).               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters in Support Vector Machine\n",
    "| Hyperparameter   | Description                                                                                                                |\n",
    "| ----------------- | -------------------------------------------------------------------------------------------------------------------------- |\n",
    "| C                | The regularization parameter that controls the trade-off between the margin and the number of training errors. A larger value of C penalizes training errors more heavily, resulting in a smaller margin but potentially better generalization performance. A smaller value of C allows for more training errors but may lead to overfitting. |\n",
    "| Kernel           | The kernel function that defines the similarity between data points. Different kernels can capture different relationships between data points, and the choice of kernel can significantly impact the performance of the SVM. Common kernels include linear, polynomial, radial basis function (RBF), and sigmoid. |\n",
    "| Gamma            | The parameter that controls the influence of support vectors on the decision boundary. A larger value of gamma indicates that nearby support vectors have a stronger influence, while a smaller value indicates that distant support vectors have a weaker influence. The choice of gamma is particularly important for RBF kernels. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Models can have many hyperparameters and finding the best combination of parameters can be treated as a search problem.\n",
    "  \n",
    "- The two strategies for Hyperparameter tuning are:\n",
    "  1. GridSearchCV\n",
    "  2. RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GridSearchCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid search is a brute force approach to hyperparameter optimization.\n",
    "  \n",
    "- It explores all possible combinations from a grid of hyperparameter values.\n",
    "  \n",
    "- Each set's model performance is logged, and the combination with the best results is chosen.\n",
    "  \n",
    "- GridSearchCV refers to this approach, searching for the best hyperparameter set from the grid.\n",
    "  \n",
    "- Despite being exhaustive and ideal for finding the best combination, grid search is slow.\n",
    "  \n",
    "- It requires significant processing power and time, which may not always be available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For example:**\n",
    "\n",
    "If we want to set two hyperparameters `C` and `Alpha` of the Logistic Regression Classifier model, with different sets of values.\n",
    "\n",
    "The grid search technique will construct many versions of the model with all possible combinations of hyperparameters and will return the best one.\n",
    "\n",
    "As in the image:\n",
    "\n",
    "- `C` values: [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "- `Alpha` values: [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "For a combination of `C=0.3` and `Alpha=0.2`, the performance score comes out to be 0.726 (highest), therefore it is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![merge3cluster](https://media.geeksforgeeks.org/wp-content/uploads/Hyp_tune.png)\n",
    "\n",
    "Source: Geeksforgeeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GridSearchCV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.006105402296585327}\n",
      "Best score is 0.853\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "\tn_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Creating the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiating logistic regression classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiating the GridSearchCV object\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Assuming X and y are your feature matrix and target variable\n",
    "# Fit the GridSearchCV object to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawback:**\n",
    "\n",
    "GridSearchCV will go through all the intermediate combinations of hyperparameters which makes grid search computationally very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RandomizedSearchCV \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Random Search Method:**\n",
    "  - Selects values randomly, contrasting with the predetermined set of numbers in grid search.\n",
    "\n",
    "  - Attempts a different set of hyperparameters in each iteration and logs the model's performance.\n",
    "  \n",
    "  - Returns the combination with the best outcome after several iterations, reducing unnecessary computation.\n",
    "\n",
    "<br/>\n",
    "\n",
    "- **RandomizedSearchCV:**\n",
    "  - Addresses drawbacks of GridSearchCV by exploring a fixed number of hyperparameter settings.\n",
    "\n",
    "  - Moves within the grid in a random fashion to find the best set of hyperparameters.\n",
    "\n",
    "  - Generally produces comparable results faster than a grid search in most cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RandomizedSearchCV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'min_samples_leaf': 1}\n",
      "Best score is 0.825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a synthetic dataset for illustration\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Rest of your code (including the RandomizedSearchCV part)\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "\t\"max_depth\": [3, None],\n",
    "\t\"max_features\": randint(1, 9),\n",
    "\t\"min_samples_leaf\": randint(1, 9),\n",
    "\t\"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawback:**\n",
    "\n",
    "Itâ€™s possible that the outcome could not be the ideal hyperparameter combination is a disadvantage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dealing with High-Dimensional Hyperparameter Spaces:** Efficient Exploration and Optimization\n",
    "  \n",
    "- **Handling Expensive Function Evaluations:** Balancing Computational Efficiency and Accuracy\n",
    "  \n",
    "- **Incorporating Domain Knowledge:** Utilizing Prior Information for Informed Tuning\n",
    "\n",
    "- **Developing Adaptive Hyperparameter Tuning Methods:** Adjusting Parameters During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Selection:** Choosing the Right Model Architecture for the Task\n",
    "\n",
    "- **Regularization Parameter Tuning:** Controlling Model Complexity for Optimal Performance\n",
    "\n",
    "- **Feature Preprocessing Optimization:** Enhancing Data Quality and Model Performance\n",
    "\n",
    "- **Algorithmic Parameter Tuning:** Adjusting Algorithm-Specific Parameters for Optimal Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages of Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Advantages** | **Disadvantages** |\n",
    "| -------------------------------------- | ---------------------------------------- |\n",
    "| - Improved model performance           | - Computational cost                   |\n",
    "| - Reduced overfitting and underfitting  | - Time-consuming process               |\n",
    "| - Enhanced model generalizability       | - Risk of overfitting                   |\n",
    "| - Optimized resource utilization        | - No guarantee of optimal performance   |\n",
    "| - Improved model interpretability      | - Requires expertise                   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
