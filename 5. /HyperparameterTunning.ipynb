{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cross Validation in Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data.\n",
    "\n",
    "- It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds.\n",
    "\n",
    "- This process is repeated multiple times, each time using a different fold as the validation set.\n",
    "\n",
    "- Finally, the results from each validation step are averaged to produce a more robust estimate of the model’s performance.\n",
    "\n",
    "- Cross validation is an important step in the machine learning process and helps to ensure that the model selected for deployment is robust and generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is cross-validation used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main purpose of cross validation is to prevent overfitting, which occurs when a model is trained too well on the training data and performs poorly on new, unseen data.\n",
    "\n",
    "- By evaluating the model on multiple validation sets, cross validation provides a more realistic estimate of the model’s generalization performance, i.e., its ability to perform well on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are several types of cross validation techniques\n",
    "  - k-fold cross validation,\n",
    "  - leave-one-out cross validation,\n",
    "  - Holdout validation,\n",
    "  - Stratified Cross-Validation\n",
    "\n",
    "- The choice of technique depends on the size and nature of the data, as well as the specific requirements of the modeling problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In K-Fold Cross Validation, we split the dataset into k number of subsets (known as folds) then we perform training on the all the subsets but leave one(k-1) subset for the evaluation of the trained model.\n",
    "\n",
    "- In this method, we iterate k times with a different subset reserved for testing purpose each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of K Fold Cross Validation*\n",
    "\n",
    "- The diagram below shows an example of the training subsets and evaluation subsets generated in k-fold cross-validation.\n",
    "\n",
    "- Here, we have total 25 instances.\n",
    "- In first iteration we use the first 20 percent of data for evaluation, and the remaining 80 percent for training ([1-5] testing and [5-25] training) while in the second iteration we use the second subset of 20 percent for evaluation, and the remaining three subsets of the data for training ([5-10] testing and [1-5 and 10-25] training), and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![merge3cluster](https://media.geeksforgeeks.org/wp-content/uploads/crossValidation.jpg)\n",
    "\n",
    "Source: GeeksforGeeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Total instances: 25\n",
    "Value of k     : 5 \n",
    "No. Iteration              Training set observations                     Testing set observations\n",
    " 1      [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]   [0 1 2 3 4]\n",
    " 2      [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]   [5 6 7 8 9]\n",
    " 3      [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]   [10 11 12 13 14]\n",
    " 4      [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]   [15 16 17 18 19]\n",
    " 5      [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]   [20 21 22 23 24]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*k fold cross-validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results (Accuracy): [1.         1.         0.96666667 0.93333333 0.96666667]\n",
      "Mean Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "cross_val_results = cross_val_score(svm_classifier, X, y, cv=kf)\n",
    "\n",
    "print(f'Cross-Validation Results (Accuracy): {cross_val_results}')\n",
    "print(f'Mean Accuracy: {cross_val_results.mean()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages of Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Advantages**                                      | **Disadvantages**                                   |\n",
    "| ----------------------------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| **Overcoming Overfitting:** Prevents overfitting by providing a robust estimate of the model's performance on unseen data.              | - **Computationally Expensive:** Can be computationally expensive, especially with a large number of folds or a complex model.           |\n",
    "| - **Model Selection:** Compares different models and selects the one performing the best on average.                                    | - **Time-Consuming:** Can be time-consuming, especially with many hyperparameters to tune or when comparing multiple models.             |\n",
    "| - **Hyperparameter Tuning:** Optimizes hyperparameters, such as the regularization parameter, by selecting values resulting in the best performance on the validation set. | - **Bias-Variance Tradeoff:** The choice of the number of folds can impact the bias-variance tradeoff. Too few folds may result in high variance, while too many may result in high bias. |\n",
    "| - **Data Efficient:** Utilizes all available data for both training and validation, making it more data-efficient compared to traditional validation techniques. |                                                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Machine Learning model is defined as a mathematical model with several parameters that need to be learned from the data.\n",
    "\n",
    "- By training a model with existing data, we can fit the model parameters.\n",
    "\n",
    "-  However, there is another kind of parameter, known as `Hyperparameters`, that cannot be directly learned from the regular training process.\n",
    "  \n",
    "-  They are usually fixed before the actual training process begins.\n",
    "  \n",
    "-  These parameters express important properties of the model such as its *complexity* or *how fast* it should learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameter tuning is the <u>process of selecting the optimal values for a machine learning model’s hyperparameters.</u>\n",
    "\n",
    "- Hyperparameters are settings that control the learning process of the model, such as the **learning rate**, the **number of neurons** in a neural network, or the **kernel size** in a support vector machine.\n",
    "\n",
    "- The goal of hyperparameter tuning is to find the values that lead to the best performance on a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameters in machine learning are configuration variables set before model training.\n",
    "  \n",
    "- They control the learning process, unlike model parameters learned from the data.\n",
    "  \n",
    "- Hyperparameters are crucial for tuning a model's performance and can impact accuracy, generalization, and other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Ways of Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameters differ from model parameters (weights and biases) learned from the data.\n",
    "  \n",
    "- Various types of hyperparameters exist, each with specific roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters in Neural Networks\n",
    "| Hyperparameter           | Description                                                                                                                |\n",
    "|--------------------------|----------------------------------------------------------------------------------------------------------------------------|\n",
    "| Learning rate            | Controls the step size taken by the optimizer during each training iteration. Too small or large rates can lead to convergence issues.           |\n",
    "| Epochs                   | Represents the number of times the entire training dataset passes through the model during training. Increased epochs may enhance performance but could lead to overfitting.    |\n",
    "| Number of layers         | Determines the depth of the model, impacting complexity and learning ability.                                              |\n",
    "| Number of nodes per layer| Influences the width of the model, affecting its capacity to represent complex relationships in the data.                   |\n",
    "| Architecture             | Dictates the overall structure of the neural network, including the number of layers, neurons per layer, and connections. Optimal architecture depends on task complexity and dataset size. |\n",
    "| Activation function      | Introduces non-linearity, enabling the model to learn complex decision boundaries. Common functions include sigmoid, tanh, and Rectified Linear Unit (ReLU).               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters in Support Vector Machine\n",
    "| Hyperparameter   | Description                                                                                                                |\n",
    "| ----------------- | -------------------------------------------------------------------------------------------------------------------------- |\n",
    "| C                | The regularization parameter that controls the trade-off between the margin and the number of training errors. A larger value of C penalizes training errors more heavily, resulting in a smaller margin but potentially better generalization performance. A smaller value of C allows for more training errors but may lead to overfitting. |\n",
    "| Kernel           | The kernel function that defines the similarity between data points. Different kernels can capture different relationships between data points, and the choice of kernel can significantly impact the performance of the SVM. Common kernels include linear, polynomial, radial basis function (RBF), and sigmoid. |\n",
    "| Gamma            | The parameter that controls the influence of support vectors on the decision boundary. A larger value of gamma indicates that nearby support vectors have a stronger influence, while a smaller value indicates that distant support vectors have a weaker influence. The choice of gamma is particularly important for RBF kernels. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Models can have many hyperparameters and finding the best combination of parameters can be treated as a search problem.\n",
    "  \n",
    "- The two strategies for Hyperparameter tuning are:\n",
    "  1. GridSearchCV\n",
    "  2. RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GridSearchCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid search is a brute force approach to hyperparameter optimization.\n",
    "  \n",
    "- It explores all possible combinations from a grid of hyperparameter values.\n",
    "  \n",
    "- Each set's model performance is logged, and the combination with the best results is chosen.\n",
    "  \n",
    "- GridSearchCV refers to this approach, searching for the best hyperparameter set from the grid.\n",
    "  \n",
    "- Despite being exhaustive and ideal for finding the best combination, grid search is slow.\n",
    "  \n",
    "- It requires significant processing power and time, which may not always be available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For example:**\n",
    "\n",
    "If we want to set two hyperparameters `C` and `Alpha` of the Logistic Regression Classifier model, with different sets of values.\n",
    "\n",
    "The grid search technique will construct many versions of the model with all possible combinations of hyperparameters and will return the best one.\n",
    "\n",
    "As in the image:\n",
    "\n",
    "- `C` values: [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "- `Alpha` values: [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "For a combination of `C=0.3` and `Alpha=0.2`, the performance score comes out to be 0.726 (highest), therefore it is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![merge3cluster](https://media.geeksforgeeks.org/wp-content/uploads/Hyp_tune.png)\n",
    "\n",
    "Source: Geeksforgeeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*GridSearchCV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameters: {'C': 0.006105402296585327}\n",
      "Best score is 0.853\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "\tn_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Creating the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiating logistic regression classifier\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiating the GridSearchCV object\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Assuming X and y are your feature matrix and target variable\n",
    "# Fit the GridSearchCV object to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawback:**\n",
    "\n",
    "GridSearchCV will go through all the intermediate combinations of hyperparameters which makes grid search computationally very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RandomizedSearchCV \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Random Search Method:**\n",
    "  - Selects values randomly, contrasting with the predetermined set of numbers in grid search.\n",
    "\n",
    "  - Attempts a different set of hyperparameters in each iteration and logs the model's performance.\n",
    "  \n",
    "  - Returns the combination with the best outcome after several iterations, reducing unnecessary computation.\n",
    "\n",
    "<br/>\n",
    "\n",
    "- **RandomizedSearchCV:**\n",
    "  - Addresses drawbacks of GridSearchCV by exploring a fixed number of hyperparameter settings.\n",
    "\n",
    "  - Moves within the grid in a random fashion to find the best set of hyperparameters.\n",
    "\n",
    "  - Generally produces comparable results faster than a grid search in most cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RandomizedSearchCV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'min_samples_leaf': 1}\n",
      "Best score is 0.825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a synthetic dataset for illustration\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Rest of your code (including the RandomizedSearchCV part)\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "\t\"max_depth\": [3, None],\n",
    "\t\"max_features\": randint(1, 9),\n",
    "\t\"min_samples_leaf\": randint(1, 9),\n",
    "\t\"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawback:**\n",
    "\n",
    "It’s possible that the outcome could not be the ideal hyperparameter combination is a disadvantage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dealing with High-Dimensional Hyperparameter Spaces:** Efficient Exploration and Optimization\n",
    "  \n",
    "- **Handling Expensive Function Evaluations:** Balancing Computational Efficiency and Accuracy\n",
    "  \n",
    "- **Incorporating Domain Knowledge:** Utilizing Prior Information for Informed Tuning\n",
    "\n",
    "- **Developing Adaptive Hyperparameter Tuning Methods:** Adjusting Parameters During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Selection:** Choosing the Right Model Architecture for the Task\n",
    "\n",
    "- **Regularization Parameter Tuning:** Controlling Model Complexity for Optimal Performance\n",
    "\n",
    "- **Feature Preprocessing Optimization:** Enhancing Data Quality and Model Performance\n",
    "\n",
    "- **Algorithmic Parameter Tuning:** Adjusting Algorithm-Specific Parameters for Optimal Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages & Disadvantages of Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Advantages** | **Disadvantages** |\n",
    "| -------------------------------------- | ---------------------------------------- |\n",
    "| - Improved model performance           | - Computational cost                   |\n",
    "| - Reduced overfitting and underfitting  | - Time-consuming process               |\n",
    "| - Enhanced model generalizability       | - Risk of overfitting                   |\n",
    "| - Optimized resource utilization        | - No guarantee of optimal performance   |\n",
    "| - Improved model interpretability      | - Requires expertise                   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
